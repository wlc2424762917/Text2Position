"""Module to perform hints-to-objects matching through a Transformer followed by Sinkhorn iterations.
Unlike SuperGlue, this did not show promising results, only kept for the sake of completion.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import numpy as np
import os
import pickle
from easydict import EasyDict

from models.modules import get_mlp, LanguageEncoder


class TransformerMatch1(torch.nn.Module):
    def __init__(
        self, known_classes, known_words, embedding_dim, num_layers, nhead, dim_feedforward
    ):
        super(TransformerMatch1, self).__init__()

        # Set idx=0 for padding
        self.known_classes = {c: (i + 1) for i, c in enumerate(known_classes)}
        self.known_classes["<unk>"] = 0
        self.class_embedding = nn.Embedding(len(self.known_classes), embedding_dim, padding_idx=0)

        # TODO: optimize these layers
        # self.pos_embedding = get_mlp([2,4,8,16,embedding_dim])
        self.pos_embedding = get_mlp([2, 128, embedding_dim])

        self.language_encoder = LanguageEncoder(known_words, embedding_dim, bi_dir=True)

        # Self attention layers
        # self.encoder_layers = nn.ModuleList([nn.TransformerEncoderLayer(2 * embedding_dim, nhead=8, dim_feedforward=2048) for i in range(num_layers)])
        self.encoder_layers = nn.ModuleList(
            [
                nn.TransformerEncoderLayer(
                    2 * embedding_dim, nhead=nhead, dim_feedforward=dim_feedforward
                )
                for i in range(num_layers)
            ]
        )
        # TODO: do Xavier?
        for layer in self.encoder_layers:
            for p in layer.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)

        # MLPs
        self.mlp_ref_confidence = get_mlp([2 * embedding_dim, embedding_dim, 1])

        self.mlp_target_class = get_mlp(
            [embedding_dim, embedding_dim // 2, len(self.known_classes)]
        )

        self.mlp_object_class = get_mlp([2 * embedding_dim, embedding_dim, len(self.known_classes)])

        self.mlp_mentioned = get_mlp([2 * embedding_dim, embedding_dim, 1])

        # self.mlp_object_offset = get_mlp([2*embedding_dim, embedding_dim, 64, 2])

    # TODO: if objects are the same, feed them in once or as full batch? -> Probably as batch for separate aux.-losses
    """
    Objects as [obj1, obj2, ...], assumed to be the same for all descriptions
    Descriptions as batch [d1, d2, d3, ..., d_B] with d_i a string. Strings can be of different sizes.
    ## Object referance types [B, num_obj âˆˆ {0,1,2}: ground-truth whether each object is unrelated, mentioned, or the target. Used for aux.-loss.
    """

    def forward(self, object_classes, object_positions, descriptions, do_print=False):
        """
        Encode the descriptions
        """
        batch_size = len(descriptions)
        description_encodings = self.language_encoder(descriptions)  # [B, DIM]
        description_encodings = torch.unsqueeze(description_encodings, dim=1)  # [B, 1, DIM]

        # description_encodings = F.relu(description_encodings) #TODO: do this or not?

        """
        Encode the objects
        """
        num_objects = len(object_classes[0])
        class_indices = torch.zeros((batch_size, num_objects), dtype=torch.long)
        for i in range(batch_size):
            for j in range(num_objects):
                class_indices[i, j] = self.known_classes.get(object_classes[i][j], 0)
        class_embeddings = self.class_embedding(class_indices.to(self.device))  # [B, num_obj, DIM]

        pos_embeddings = self.pos_embedding(
            torch.tensor(object_positions, dtype=torch.float, device=self.device)
        )  # [B, num_obj, DIM]

        object_encodings = class_embeddings + pos_embeddings  # [B, num_obj, DIM]
        if do_print:
            print()
            print(
                "class",
                torch.mean(torch.abs(class_embeddings)).item(),
                "pos",
                torch.mean(torch.abs(pos_embeddings)).item(),
            )
            print()

        # TODO: norm somewhere?
        """
        Merge object and description encodings (concat the description encoding to every object encoding for combined transformer inputs)
        """
        description_encodings_repeated = description_encodings.repeat(
            1, num_objects, 1
        )  # [B, num_obj, DIM]
        transformer_input = torch.cat(
            (object_encodings, description_encodings_repeated), dim=-1
        )  # [B, num_obj, 2*DIM]

        """
        Run Tranformer Encoder Layers
        """
        # features = torch.transpose(transformer_input, 0, 1) # [B, num_obj, E] -> [num_obj, B, E]
        features = transformer_input  # TODO: transpose or not?!
        for layer in self.encoder_layers:
            features = layer(features)

        """
        Make predictions
        """
        obj_ref_pred = self.mlp_ref_confidence(features)  # [num_obj, B, 1]
        # obj_ref_pred = torch.transpose(torch.squeeze(obj_ref_pred, dim=-1), 0, 1) # [num_obj, B, 1] -> [B, num_obj]
        obj_ref_pred = torch.squeeze(obj_ref_pred, dim=-1)

        target_class_pred = torch.squeeze(self.mlp_target_class(description_encodings), dim=1)

        obj_class_pred = self.mlp_object_class(features)

        obj_mentioned_pred = self.mlp_mentioned(features)

        # obj_offset_pred = self.mlp_object_offset(features)

        model_output = EasyDict()
        model_output.features = features
        model_output.obj_ref_pred = obj_ref_pred
        model_output.target_class_pred = target_class_pred
        model_output.obj_class_pred = obj_class_pred
        model_output.obj_mentioned_pred = obj_mentioned_pred
        # model_output.obj_offset_pred = obj_offset_pred

        return model_output

    @property
    def device(self):
        return next(self.pos_embedding.parameters()).device


if __name__ == "__main__":
    model = TransformerMatch1(
        ["high vegetation", "low vegetation", "buildings", "hard scape", "cars"],
        "a b c d e".split(),
        300,
        2,
    )

    model(
        [
            ["high vegetation", "low vegetation", "buildings", "hard scape", "cars", "xx"]
            for i in range(3)
        ],
        ["a b x d e", "a b c x a c", "a a"],
    )
